{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nemanja Petrovic SIR 1 - SRBerta pre train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSCAR Dataset for Serbian language\n",
    "\n",
    "We need to download data from OSCAR corpus from Hugging face, after data is downloaded we are removing all new lines and divide data to chucnks of 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T08:36:23.722190Z",
     "start_time": "2023-05-09T08:21:39.235148400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/22.3k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "283b35ccf6014ac3b824859fd469753f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/37.4k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "222e4ea6572842c4bf31ce1fa918960a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset oscar-2301/sr to C:/Users/HP1/Documents/Nemanja/SRBerta-pretrain/dataset_cache/oscar-corpus___oscar-2301/sr-language=sr/0.0.0/156efb8ba9f439f881d8f41fd7fddd5e04604bc27505c46ddef015f2fc551a4a...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/450 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bcb17fd3305438693631040b0cc57aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdde8d3942f74e8b8ac35f3c115f155c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/532M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e48632968cd47f3aa554548db66ddbe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/344M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "029a0e1895c34436b52a7c521b934456"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/531M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bcde5ff6e5c400e920a7eefe5e2fdbb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/532M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ade1994281a442a0aeb61f3cf13e7e11"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/531M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d40bc83bfe6474692e37f9f4f84d4b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9679b359fa294bb685c058124ba85b15"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset oscar-2301 downloaded and prepared to C:/Users/HP1/Documents/Nemanja/SRBerta-pretrain/dataset_cache/oscar-corpus___oscar-2301/sr-language=sr/0.0.0/156efb8ba9f439f881d8f41fd7fddd5e04604bc27505c46ddef015f2fc551a4a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97c12ebd9b6a4581bb595ac0602e292e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'text', 'meta'],\n",
      "        num_rows: 838948\n",
      "    })\n",
      "})\n",
      "STARTED WRITING DATA TO FILES\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/838948 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14e2e81172594c5ba7bd34e9d483baee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED WRITING DATA TO FILES\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hugging_face_token = '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'\n",
    "dataset = load_dataset(\"oscar-corpus/OSCAR-2301\",\n",
    "                       cache_dir=\"dataset_cache\",\n",
    "                       use_auth_token=hugging_face_token,\n",
    "                       language=\"sr\",\n",
    "                       streaming=False)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# Format everything and pu in files with length 10000\n",
    "print(\"STARTED WRITING DATA TO FILES\")\n",
    "from tqdm.auto import tqdm\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(dataset['train']):\n",
    "\n",
    "    sample = sample['text'].replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "\n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'sr_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "\n",
    "# after saving in 10K chunks, we have to add leftovers\n",
    "with open(f'sr_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))\n",
    "\n",
    "print(\"FINISHED WRITING DATA TO FILES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "Before training model, we need to train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T09:35:36.429488400Z",
     "start_time": "2023-05-09T08:43:13.391269700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TOKENIZER TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED TOKENIZER TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input ids in sample:\n",
      "torch.Size([1, 10])\n",
      "Testing decoder\n"
     ]
    },
    {
     "data": {
      "text/plain": "' дан'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from transformers import RobertaTokenizerFast\n",
    "from tokenizers.decoders import ByteLevel\n",
    "\n",
    "paths = [str(x) for x in Path('./').glob('*.txt')]\n",
    "# For testing, taking only first 5 files, for real training remove this and go through more data\n",
    "paths = paths[0:60]\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "print(\"STARTING TOKENIZER TRAINING\")\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(\n",
    "    files=paths,\n",
    "    vocab_size=50000,\n",
    "    min_frequency=2,show_progress=True,\n",
    "    special_tokens=[\n",
    "        '<s>', '<pad>', '</s>', '<unk>', '<mask>'\n",
    "    ]\n",
    ")\n",
    "print(\"FINISHED TOKENIZER TRAINING\")\n",
    "\n",
    "os.mkdir('./srberta_tokenizer')\n",
    "tokenizer.save_model('srberta_tokenizer')\n",
    "srberta_tokenizer = RobertaTokenizerFast.from_pretrained(\"srberta_tokenizer\")\n",
    "\n",
    "sample = srberta_tokenizer(\"Добар дан, како си данас ти човече\", return_tensors='pt')\n",
    "print(\"Shape of input ids in sample:\")\n",
    "print(str(sample.input_ids.shape))\n",
    "\n",
    "# Test decoder\n",
    "print(\"Testing decoder\")\n",
    "decoder = ByteLevel()\n",
    "decoder.decode('ĠÐ´Ð°Ð½')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T12:18:03.419783100Z",
     "start_time": "2023-05-09T10:15:21.962438100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/84 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2acee1f0565d41519068f3e1c0f016fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tensor([    0,  6928,  7624,     4,  1606,   910,  5078, 19558,   365,   480])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizerFast\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "def mlm(tensor):\n",
    "\n",
    "    rand = torch.rand(tensor.shape) #[0,1]\n",
    "    mask_arr = (rand < 0.15)* (tensor!=0)* (tensor!=1)* (tensor!=2)\n",
    "    for i in range(tensor.shape[0]):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        tensor[i, selection] = 4\n",
    "\n",
    "    return tensor\n",
    "\n",
    "paths = [str(x) for x in Path('./').glob('*.txt')]\n",
    "\n",
    "tokenizer_srberta = RobertaTokenizerFast.from_pretrained(\"srberta_tokenizer\")\n",
    "\n",
    "input_ids = []\n",
    "mask = [] # attention mask\n",
    "labels = []\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "\n",
    "    sample = tokenizer_srberta(lines, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "    labels.append(sample.input_ids)\n",
    "    mask.append(sample.attention_mask)\n",
    "    input_ids.append(mlm(sample.input_ids.detach().clone()))\n",
    "\n",
    "# sample['input_ids'].shape\n",
    "\n",
    "input_ids = torch.cat(input_ids)\n",
    "mask = torch.cat(mask)\n",
    "labels = torch.cat(labels)\n",
    "torch.save(input_ids, 'input_ids.pt')\n",
    "torch.save(mask, 'mask.pt')\n",
    "torch.save(labels, 'labels.pt')\n",
    "\n",
    "input_ids = torch.load(\"input_ids.pt\")\n",
    "mask = torch.load(\"mask.pt\")\n",
    "labels = torch.load(\"labels.pt\")\n",
    "input_ids[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "(7, 5)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_capability()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T08:21:22.067698900Z",
     "start_time": "2023-05-09T08:21:17.506244200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'input_ids': tensor([[    0,     4,  5527,  ..., 11686,    18,     2],\n",
      "        [    0, 15478,  1320,  ...,     4, 22407,     2],\n",
      "        [    0, 14480, 32316,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,   547,  7060,  ...,     1,     1,     1],\n",
      "        [    0,    12,     4,  ..., 27411,  2242,     2],\n",
      "        [    0,   449,   505,  ...,   341, 10476,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[    0, 34895,  5527,  ..., 11686,    18,     2],\n",
      "        [    0, 15478,  1320,  ..., 27231, 22407,     2],\n",
      "        [    0, 14480, 32316,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,   547,  7060,  ...,     1,     1,     1],\n",
      "        [    0,    12, 34278,  ..., 27411,  2242,     2],\n",
      "        [    0,   449,   505,  ...,   341, 10476,     2]])}\n",
      "838948\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = torch.load(\"input_ids.pt\")\n",
    "mask = torch.load(\"mask.pt\")\n",
    "labels = torch.load(\"labels.pt\")\n",
    "\n",
    "encodings = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': mask,\n",
    "    'labels': labels\n",
    "}\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}\n",
    "\n",
    "dataset = Dataset(encodings)\n",
    "BATCH_SIZE = 16\n",
    "DO_SHUFFLE = True\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=DO_SHUFFLE)\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    print(i)\n",
    "    print(data)\n",
    "    break\n",
    "\n",
    "print(len(dataloader.dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-10T06:47:56.893463900Z",
     "start_time": "2023-05-10T06:47:51.255870600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/52435 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5e7dee7189b4fdbb6047962a2ca369e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 49\u001B[0m\n\u001B[0;32m     46\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(input_ids, attention_mask\u001B[38;5;241m=\u001B[39mmask, labels\u001B[38;5;241m=\u001B[39mlabels)\n\u001B[0;32m     47\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m---> 49\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m optim\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     52\u001B[0m loop\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\conda-env-sir1-windows\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\conda-env-sir1-windows\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import RobertaForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tokenizer_srberta = RobertaTokenizerFast.from_pretrained(\"srberta_tokenizer\")\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=tokenizer_srberta.vocab_size,\n",
    "    max_position_embeddings=514,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config) #randomly initialized weights\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device.cpu()\n",
    "print(str(device))\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=1e-4)\n",
    "epochs=1\n",
    "\n",
    "writer = SummaryWriter(\"./runs_v2\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    step=0\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "\n",
    "    for batch in loop:\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loop.set_description(f'Epoch: {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", loss, step)\n",
    "        writer.flush()\n",
    "\n",
    "        if step % 10_000 == 0:\n",
    "            torch.save({'optimizer_state_dict': optim.state_dict()}, str(step)+'_'+ str(epoch)+'_optimizer.pt')\n",
    "            model.save_pretrained(\"./srberta_model_\"+str(step)+'_'+ str(epoch))\n",
    "\n",
    "        step+=1\n",
    "\n",
    "    # Save after each epoch\n",
    "    torch.save({'optimizer_state_dict': optim.state_dict()}, str(epoch)+'_optimizer.pt')\n",
    "    model.save_pretrained(\"./srberta_model_\"+ str(epoch))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-10T06:48:25.270881600Z",
     "start_time": "2023-05-10T06:48:04.904769Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m torch\u001B[38;5;241m.\u001B[39msave({\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer_state_dict\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[43moptim\u001B[49m\u001B[38;5;241m.\u001B[39mstate_dict()\n\u001B[0;32m      4\u001B[0m },\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer_3_epochs.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m model\u001B[38;5;241m.\u001B[39msave_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./srberta_model_3_epochs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave\u001B[39m(model, optimizer):\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# save\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.save({\n",
    "    'optimizer_state_dict': optim.state_dict()\n",
    "},'optimizer_3_epochs.pt')\n",
    "\n",
    "model.save_pretrained(\"./srberta_model_3_epochs\")\n",
    "\n",
    "def save(model, optimizer):\n",
    "    # save\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, 'output_model.pt')\n",
    "\n",
    "save(model, optim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-10T06:43:55.415597200Z",
     "start_time": "2023-05-10T06:42:39.087569500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': 0.01705239899456501,\n  'token': 18,\n  'token_str': '.',\n  'sequence': 'Добар дан како. '},\n {'score': 0.0163358673453331,\n  'token': 341,\n  'token_str': ' на',\n  'sequence': 'Добар дан како на '},\n {'score': 0.01422953512519598,\n  'token': 316,\n  'token_str': ' је',\n  'sequence': 'Добар дан како је '},\n {'score': 0.013286540284752846,\n  'token': 16,\n  'token_str': ',',\n  'sequence': 'Добар дан како, '},\n {'score': 0.011648965999484062,\n  'token': 280,\n  'token_str': ' и',\n  'sequence': 'Добар дан како и '}]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model_v2 = RobertaForMaskedLM.from_pretrained(\"./srberta_model_1\")\n",
    "model_v2.to('cpu')\n",
    "\n",
    "fill = pipeline('fill-mask', model=model_v2, tokenizer=tokenizer_srberta)\n",
    "fill(f'Добар дан како {fill.tokenizer.mask_token} ')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:42:07.831333Z",
     "end_time": "2023-04-24T22:42:08.495064Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
