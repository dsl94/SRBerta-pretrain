{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nemanja Petrovic SIR 1 - SRBerta pre train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSCAR Dataset for Serbian language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset oscar-2301/sr to /Users/nemanja/Documents/elfak/SIR 1/srberta-pretrain/dataset_cache/oscar-corpus___oscar-2301/sr-language=sr/0.0.0/156efb8ba9f439f881d8f41fd7fddd5e04604bc27505c46ddef015f2fc551a4a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9d82ce77b549e08469fa5d1b7b51d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0752c60dbc9e4f5a99a350bbcdfa74c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ff38b6bc6e43d587fa3175b8311cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/532M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06f41a268094475b553ad21fa4139c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/344M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80322be31cee4c5eba3c9bf7102aac3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/531M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eea6f1528f44eae9acc4355c66c8849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/532M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4b101ddf2341bdb9813faa2f3e312d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/531M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d016d7655e2b45f1b051eea02f1f7953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset oscar-2301 downloaded and prepared to /Users/nemanja/Documents/elfak/SIR 1/srberta-pretrain/dataset_cache/oscar-corpus___oscar-2301/sr-language=sr/0.0.0/156efb8ba9f439f881d8f41fd7fddd5e04604bc27505c46ddef015f2fc551a4a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1faf4a8306e94988b9e103b4d9420ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'meta'],\n",
       "        num_rows: 838948\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hugging_face_token = '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'\n",
    "dataset = load_dataset(\"oscar-corpus/OSCAR-2301\",\n",
    "                       cache_dir=\"dataset_cache\",\n",
    "                       use_auth_token=hugging_face_token,\n",
    "                       language=\"sr\",\n",
    "                       streaming=False)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utf8len(s):\n",
    "    return len(s.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Министар спољних послова Републике Србије Иван Мркић који ове недеље борави у Њујорку, у преподневним часовима по локалном времену, присуствоваће као члан делегације председника Србије Томислава Николића, отварању генералне дебате Генералне скупштине Уједињених нација.Потом, шеф српске дипломатије има низ званичних билатералних састанака. Најпре ће разговарати са албанским колегом Дитмиром Бушатијем.Са министром спољних послова Омана, Јусуфом Бин Алавијем Бин Абдулахом, Иван Мркић ће потписати међувладин споразум.Шеф српске дипломатије, данас се састаје и са Генералним секретаром Савета Европе Торбјорном Јагландом.У 18 часова, министар спољних послова Републике Србије Иван Мркић, имаће трилатерални састанак са колегама из Турске и Босне и Херцеговине, Ахметом Давутоглуом и Златком Лагумџијом.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['text'].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38afb925a9f4d8582331270383a64fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/838948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(dataset['train']):\n",
    "\n",
    "    sample = sample['text'].replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "\n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'sr_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "\n",
    "# after saving in 10K chunks, we will have ~64 leftover samples, we save those now too\n",
    "with open(f'sr_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "paths = [str(x) for x in Path('./').glob('*.txt')]\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(\n",
    "    files=paths,\n",
    "    vocab_size=30_522,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\n",
    "        '<s>', '<pad>', '</s>', '<unk>', '<mask>'\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "os.mkdir('./srberta_tokenizer')\n",
    "tokenizer.save_model('srberta_tokenizer')\n",
    "srberta_tokenizer = RobertaTokenizerFast.from_pretrained(\"srberta_tokenizer\")\n",
    "sample = srberta_tokenizer(\"Добар дан, како си данас ти човече\", return_tensors='pt')\n",
    "sample.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 10001,   297,   773,    16,   788,  2392,  1293,  1815,  1289,\n",
       "          5139,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Доб'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.decoders import ByteLevel\n",
    "decoder = ByteLevel()\n",
    "decoder.decode('ĠÐ´Ð°Ð½')\n",
    "decoder.decode('ÐĶÐ¾Ð±')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def mlm(tensor):\n",
    "\n",
    "    rand = torch.rand(tensor.shape) #[0,1]\n",
    "    mask_arr = (rand < 0.15)* (tensor!=0)* (tensor!=1)* (tensor!=2)\n",
    "    for i in range(tensor.shape[0]):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        tensor[i, selection] = 4\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "['sr_46.txt',\n 'sr_52.txt',\n 'sr_5.txt',\n 'sr_4.txt',\n 'sr_53.txt',\n 'sr_47.txt',\n 'sr_51.txt',\n 'sr_45.txt',\n 'sr_79.txt',\n 'sr_6.txt',\n 'sr_7.txt',\n 'sr_78.txt',\n 'sr_44.txt',\n 'sr_50.txt',\n 'sr_83.txt',\n 'sr_68.txt',\n 'sr_54.txt',\n 'sr_40.txt',\n 'sr_3.txt',\n 'sr_2.txt',\n 'sr_41.txt',\n 'sr_55.txt',\n 'sr_69.txt',\n 'sr_82.txt',\n 'sr_80.txt',\n 'sr_43.txt',\n 'sr_57.txt',\n 'sr_0.txt',\n 'sr_1.txt',\n 'sr_56.txt',\n 'sr_42.txt',\n 'sr_81.txt',\n 'sr_25.txt',\n 'sr_31.txt',\n 'sr_19.txt',\n 'sr_18.txt',\n 'sr_30.txt',\n 'sr_24.txt',\n 'sr_32.txt',\n 'sr_26.txt',\n 'sr_27.txt',\n 'sr_33.txt',\n 'sr_37.txt',\n 'sr_23.txt',\n 'sr_22.txt',\n 'sr_36.txt',\n 'sr_20.txt',\n 'sr_34.txt',\n 'sr_35.txt',\n 'sr_21.txt',\n 'sr_10.txt',\n 'sr_38.txt',\n 'sr_39.txt',\n 'sr_11.txt',\n 'sr_13.txt',\n 'sr_12.txt',\n 'sr_16.txt',\n 'sr_17.txt',\n 'sr_29.txt',\n 'sr_15.txt',\n 'sr_14.txt',\n 'sr_28.txt',\n 'sr_67.txt',\n 'sr_73.txt',\n 'sr_72.txt',\n 'sr_66.txt',\n 'sr_70.txt',\n 'sr_64.txt',\n 'sr_58.txt',\n 'sr_59.txt',\n 'sr_65.txt',\n 'sr_71.txt',\n 'sr_49.txt',\n 'sr_75.txt',\n 'sr_61.txt',\n 'sr_60.txt',\n 'sr_74.txt',\n 'sr_48.txt',\n 'sr_62.txt',\n 'sr_76.txt',\n 'sr_9.txt',\n 'sr_8.txt',\n 'sr_77.txt',\n 'sr_63.txt']"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path('./').glob('*.txt')]\n",
    "paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer_srberta = RobertaTokenizerFast.from_pretrained(\"srberta_tokenizer\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/84 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1442449482b14c42a6eb9d00aaf7ebc4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# from pympler.asizeof import asizeof\n",
    "import os\n",
    "# import psutil\n",
    "\n",
    "input_ids = []\n",
    "mask = [] # attention mask\n",
    "labels = []\n",
    "\n",
    "# sample_counter = 0\n",
    "# counter =0\n",
    "\n",
    "for path in tqdm(paths):\n",
    "\n",
    "    #print(f\"Memory report:::: {psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2} MB\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "\n",
    "        lines = f.read().split('\\n')\n",
    "\n",
    "    sample = tokenizer_srberta(lines, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "    #         lines = []\n",
    "\n",
    "    labels.append(sample.input_ids)\n",
    "    mask.append(sample.attention_mask)\n",
    "    input_ids.append(mlm(sample.input_ids.detach().clone()))\n",
    "\n",
    "#     sample_counter+=1\n",
    "#     print(sample.input_ids.shape[0])\n",
    "#     if sample_counter % 64 == 0: # 64 files each file had 10_000 text samples of different len are now encoded to size 128\n",
    "#         # BATCH SIZE IS 64 this way\n",
    "#         input_ids = torch.cat(input_ids)\n",
    "#         mask = torch.cat(mask)\n",
    "#         labels = torch.cat(labels)\n",
    "\n",
    "#         torch.save({\"labels\": labels, \"mask\":mask, \"input_ids\": input_ids}, f'./tensors/{counter}_tensors.pt')\n",
    "\n",
    "#         input_ids = []\n",
    "#         mask = []\n",
    "#         labels = []\n",
    "\n",
    "#         counter +=1\n",
    "\n",
    "#         print(f\"Memory report after emtying tensors:::: {psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2} MB\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10000, 512])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['input_ids'].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tensors/1_tensors.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m a \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./tensors/1_tensors.pt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sir1/lib/python3.8/site-packages/torch/serialization.py:771\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001B[0m\n\u001B[1;32m    768\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    769\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 771\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m    772\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m    773\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m    774\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m    775\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m    776\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m~/anaconda3/envs/sir1/lib/python3.8/site-packages/torch/serialization.py:270\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 270\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    272\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m~/anaconda3/envs/sir1/lib/python3.8/site-packages/torch/serialization.py:251\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 251\u001B[0m     \u001B[38;5;28msuper\u001B[39m(_open_file, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './tensors/1_tensors.pt'"
     ]
    }
   ],
   "source": [
    "a = torch.load(\"./tensors/1_tensors.pt\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
